{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cohere Embed English is the marketâ€™s leading multimodal (text, image) representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering. Embed English has top performance on the HuggingFace MTEB benchmark and performs well on a variety of industries such as Finance, Legal, and General-Purpose Corpora.The model was trained on nearly 1B English training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables for API keys and configurations from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(item') = <class 'azure.ai.inference.models._models.EmbeddingItem'>\n",
      "data[0]: length=1024, [0.0013399124, -0.01576233, ..., 0.007843018, 0.000238657]\n",
      "typeof(item') = <class 'azure.ai.inference.models._models.EmbeddingItem'>\n",
      "data[1]: length=1024, [0.036590576, -0.0059547424, ..., 0.011405945, 0.004863739]\n",
      "typeof(item') = <class 'azure.ai.inference.models._models.EmbeddingItem'>\n",
      "data[2]: length=1024, [0.04196167, 0.029083252, ..., -0.0027484894, 0.0073127747]\n",
      "{'prompt_tokens': 6, 'completion_tokens': 0, 'total_tokens': 6}\n"
     ]
    }
   ],
   "source": [
    "# This code creates an embeddings client using Azure AI Inference SDK, connects to the Cohere-embed-v3-english model hosted on Github Marketplace using GitHub token,\n",
    "# and generates embeddings for three sample phrases. It then prints the embeddings vector length and first/last values\n",
    "# for each phrase, along with usage statistics.\n",
    "import os\n",
    "\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Cohere-embed-v3-english\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = EmbeddingsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token)\n",
    ")\n",
    "\n",
    "response = client.embed(\n",
    "    input=[\"first phrase\", \"second phrase\", \"third phrase\"],\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "for item in response.data:\n",
    "    length = len(item.embedding)\n",
    "    print(\n",
    "        f\"data[{item.index}]: length={length}, \"\n",
    "        f\"[{item.embedding[0]}, {item.embedding[1]}, \"\n",
    "        f\"..., {item.embedding[length-2]}, {item.embedding[length-1]}]\"\n",
    "    )\n",
    "print(response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azure.ai.inference.models import ImageEmbeddingInput\n",
    "\n",
    "# Load and encode the image\n",
    "img_obj = ImageEmbeddingInput.load(image_file=\"sample1.png\", image_format=\"png\")\n",
    "image_data = img_obj.__dict__['_data']['image']\n",
    "\n",
    "with open('image_data.json', 'w') as f:\n",
    "    json.dump(image_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: length=1024, [-0.011123657, 0.0071754456, ..., -0.0064201355, -0.019577026]\n"
     ]
    }
   ],
   "source": [
    "# image embedding\n",
    "import os\n",
    "\n",
    "from azure.ai.inference import ImageEmbeddingsClient\n",
    "from azure.ai.inference.models import ImageEmbeddingInput\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Cohere-embed-v3-multilingual\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ImageEmbeddingsClient(\n",
    "    endpoint=endpoint, \n",
    "    credential=AzureKeyCredential(token)\n",
    "    )\n",
    "\n",
    "response = client.embed(\n",
    "    model=model_name,\n",
    "    input=[ImageEmbeddingInput.load(image_file=\"sample1.png\", image_format=\"png\")]\n",
    "    )\n",
    "\n",
    "for item in response.data:\n",
    "    length = len(item.embedding)\n",
    "    print(\n",
    "        f\"data[{item.index}]: length={length}, [{item.embedding[0]}, {item.embedding[1]}, \"\n",
    "        f\"..., {item.embedding[length-2]}, {item.embedding[length-1]}]\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
